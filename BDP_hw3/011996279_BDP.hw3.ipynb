{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Platform\n",
    "## Assignment 3: ServerLess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**By:**\n",
    "Yarden Fogel\n",
    "Roy Ludan\n",
    "\n",
    "### ID#1: 011996279\n",
    "### ID#2: 032736233"
   ],
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (901559905.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"/var/folders/ps/z0ml3t995m11654nrp_lsc2cv7w2kc/T/ipykernel_62638/901559905.py\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    **By:**\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this assignment is to:**\n",
    "- Understand and practice the details of Serverless\n",
    "\n",
    "**Instructions:**\n",
    "- Students will form teams of two people each, and submit a single homework for each team.\n",
    "- The same score for the homework will be given to each member of your team.\n",
    "- Your solution is in the form of a Jupyter notebook file (with extension ipynb).\n",
    "- Images/Graphs/Tables should be submitted inside the notebook.\n",
    "- The notebook should be runnable and properly documented. \n",
    "- Please answer all the questions and include all your code.\n",
    "- You are expected to submit a clear and pythonic code.\n",
    "- You can change functions signatures/definitions.\n",
    "\n",
    "**Submission:**\n",
    "- Submission of the homework will be done via Moodle by uploading (not Zip):\n",
    "    - Jupyter Notebook\n",
    "    - 2 Log files\n",
    "    - Additional local scripts\n",
    "- The homework needs to be entirely in English.\n",
    "- The deadline for submission is on Moodle.\n",
    "- Late submission won't be allowed.\n",
    "\n",
    "  \n",
    "- In case of identical code submissions - both groups will get a Zero. \n",
    "- Some groups might be selected randomly to present their code.\n",
    "\n",
    "**Requirements:**  \n",
    "- Python 3.6 should be used.  \n",
    "- You should implement the algorithms by yourself using only basic Python libraries (such as numpy,pandas,etc.)\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading:**\n",
    "- Q0 - 10 points - Setup\n",
    "- Q1 - 40 points - Serverless MapReduceEngine\n",
    "- Q2 - 20 points - MapReduce job to calculate inverted index\n",
    "- Q3 - 30 points - Shuffle\n",
    "\n",
    "`Total: 100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.python.org/simple/\r\n",
      "Requirement already satisfied: names in ./venv/lib/python3.8/site-packages (0.3.0)\r\n",
      "Looking in indexes: https://pypi.python.org/simple/\r\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.8/site-packages (1.22.0)\r\n",
      "Looking in indexes: https://pypi.python.org/simple/\r\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.8/site-packages (1.7.3)\r\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in ./venv/lib/python3.8/site-packages (from scipy) (1.22.0)\r\n",
      "Looking in indexes: https://pypi.python.org/simple/\r\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.8/site-packages (1.3.5)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in ./venv/lib/python3.8/site-packages (from pandas) (2021.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./venv/lib/python3.8/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./venv/lib/python3.8/site-packages (from pandas) (1.22.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\r\n",
      "Looking in indexes: https://pypi.python.org/simple/\r\n",
      "Requirement already satisfied: ibm-cos-sdk in ./venv/lib/python3.8/site-packages (2.11.0)\r\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.11.0 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk) (2.11.0)\r\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.11.0 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk) (2.11.0)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk) (0.10.0)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (2.8.2)\r\n",
      "Requirement already satisfied: requests<3.0,>=2.26 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (2.27.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.26.7 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (1.26.8)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (1.16.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3.0,>=2.26->ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3.0,>=2.26->ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./venv/lib/python3.8/site-packages (from requests<3.0,>=2.26->ibm-cos-sdk-core==2.11.0->ibm-cos-sdk) (2.0.10)\r\n",
      "Looking in indexes: https://pypi.python.org/simple/\r\n",
      "Requirement already satisfied: lithops in ./venv/lib/python3.8/site-packages (2.5.8)\r\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.8/site-packages (from lithops) (3.5.1)\r\n",
      "Requirement already satisfied: PyYAML in ./venv/lib/python3.8/site-packages (from lithops) (6.0)\r\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.8/site-packages (from lithops) (2.27.1)\r\n",
      "Requirement already satisfied: python-dateutil in ./venv/lib/python3.8/site-packages (from lithops) (2.8.2)\r\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.8/site-packages (from lithops) (1.3.5)\r\n",
      "Requirement already satisfied: Click in ./venv/lib/python3.8/site-packages (from lithops) (8.0.3)\r\n",
      "Requirement already satisfied: paramiko in ./venv/lib/python3.8/site-packages (from lithops) (2.9.1)\r\n",
      "Requirement already satisfied: lxml in ./venv/lib/python3.8/site-packages (from lithops) (4.7.1)\r\n",
      "Requirement already satisfied: docker in ./venv/lib/python3.8/site-packages (from lithops) (5.0.3)\r\n",
      "Requirement already satisfied: redis in ./venv/lib/python3.8/site-packages (from lithops) (4.1.0)\r\n",
      "Requirement already satisfied: pika in ./venv/lib/python3.8/site-packages (from lithops) (1.2.0)\r\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.8/site-packages (from lithops) (4.62.3)\r\n",
      "Requirement already satisfied: tblib in ./venv/lib/python3.8/site-packages (from lithops) (1.7.0)\r\n",
      "Requirement already satisfied: ps-mem in ./venv/lib/python3.8/site-packages (from lithops) (3.12)\r\n",
      "Requirement already satisfied: seaborn in ./venv/lib/python3.8/site-packages (from lithops) (0.11.2)\r\n",
      "Requirement already satisfied: ibm-cos-sdk in ./venv/lib/python3.8/site-packages (from lithops) (2.11.0)\r\n",
      "Requirement already satisfied: kubernetes in ./venv/lib/python3.8/site-packages (from lithops) (21.7.0)\r\n",
      "Requirement already satisfied: cloudpickle in ./venv/lib/python3.8/site-packages (from lithops) (2.0.0)\r\n",
      "Requirement already satisfied: ibm-vpc in ./venv/lib/python3.8/site-packages (from lithops) (0.9.0)\r\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in ./venv/lib/python3.8/site-packages (from docker->lithops) (1.2.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests->lithops) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests->lithops) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests->lithops) (1.26.8)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./venv/lib/python3.8/site-packages (from requests->lithops) (2.0.10)\r\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.11.0 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk->lithops) (2.11.0)\r\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.11.0 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk->lithops) (2.11.0)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in ./venv/lib/python3.8/site-packages (from ibm-cos-sdk->lithops) (0.10.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.8/site-packages (from python-dateutil->lithops) (1.16.0)\r\n",
      "Requirement already satisfied: ibm-cloud-sdk-core>=3.13.2 in ./venv/lib/python3.8/site-packages (from ibm-vpc->lithops) (3.13.2)\r\n",
      "Requirement already satisfied: setuptools>=21.0.0 in ./venv/lib/python3.8/site-packages (from kubernetes->lithops) (59.3.0)\r\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./venv/lib/python3.8/site-packages (from kubernetes->lithops) (2.3.3)\r\n",
      "Requirement already satisfied: requests-oauthlib in ./venv/lib/python3.8/site-packages (from kubernetes->lithops) (1.3.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venv/lib/python3.8/site-packages (from matplotlib->lithops) (9.0.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.8/site-packages (from matplotlib->lithops) (0.11.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./venv/lib/python3.8/site-packages (from matplotlib->lithops) (3.0.6)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.8/site-packages (from matplotlib->lithops) (4.28.5)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.8/site-packages (from matplotlib->lithops) (1.22.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from matplotlib->lithops) (21.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.8/site-packages (from matplotlib->lithops) (1.3.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in ./venv/lib/python3.8/site-packages (from pandas->lithops) (2021.3)\r\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in ./venv/lib/python3.8/site-packages (from paramiko->lithops) (3.2.0)\r\n",
      "Requirement already satisfied: pynacl>=1.0.1 in ./venv/lib/python3.8/site-packages (from paramiko->lithops) (1.4.0)\r\n",
      "Requirement already satisfied: cryptography>=2.5 in ./venv/lib/python3.8/site-packages (from paramiko->lithops) (36.0.1)\r\n",
      "Requirement already satisfied: deprecated>=1.2.3 in ./venv/lib/python3.8/site-packages (from redis->lithops) (1.2.13)\r\n",
      "Requirement already satisfied: scipy>=1.0 in ./venv/lib/python3.8/site-packages (from seaborn->lithops) (1.7.3)\r\n",
      "Requirement already satisfied: cffi>=1.1 in ./venv/lib/python3.8/site-packages (from bcrypt>=3.1.3->paramiko->lithops) (1.15.0)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in ./venv/lib/python3.8/site-packages (from deprecated>=1.2.3->redis->lithops) (1.13.3)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in ./venv/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes->lithops) (4.2.4)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes->lithops) (4.8)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes->lithops) (0.2.8)\r\n",
      "Requirement already satisfied: PyJWT<3.0.0,>=2.0.1 in ./venv/lib/python3.8/site-packages (from ibm-cloud-sdk-core>=3.13.2->ibm-vpc->lithops) (2.3.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.8/site-packages (from requests-oauthlib->kubernetes->lithops) (3.1.1)\r\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.8/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->lithops) (2.21)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes->lithops) (0.4.8)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet zipfile36\n",
    "!pip install names\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install pandas\n",
    "!pip install ibm-cos-sdk\n",
    "!pip install lithops"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "\n",
    "#SQL\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "# Random last names\n",
    "import names\n",
    "\n",
    "# ML standard\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Lithops\n",
    "from lithops import FunctionExecutor\n",
    "from lithops import Storage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "random.seed(123)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "format = \"%(asctime)s: %(message)s\"\n",
    "logging.basicConfig(format=format, level=logging.DEBUG, filename = './MapReduce.log', datefmt=\"%H:%M:%S\", filemode='a')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 0\n",
    "## Setup\n",
    "\n",
    "1. Navigate to IBM Cloud and open a trial account. No need to provide a credit card\n",
    "2. Choose IBM Cloud Object Storage service from the catalog\n",
    "3. Create a new bucket in IBM Cloud Object Storage\n",
    "4. Create credentials for the bucket with HMAC (access key and secret key)\n",
    "5. Choose IBM Cloud Functions service from the catalog and create a service\n",
    "\n",
    "\n",
    "#### Lithops setup\n",
    "1. By using “git” tool, install master branch of the Lithops project from\n",
    "https://github.com/lithops-cloud/lithops\n",
    "2. Follow Lithops documentation and configure Lithops against IBM Cloud Functions and IBM Cloud Object Storage\n",
    "3. Configure Lithops log level to be in DEBUG mode\n",
    "4. Run Hello World example by using Futures API and verify all is working properly.\n",
    "\n",
    "\n",
    "#### IBM Cloud Object Storage setup\n",
    "1. Upload all the input CSV files that you used in homework 2 into the bucket you created in IBM Cloud Object Storage\n",
    "\n",
    "\n",
    "<br><br><br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "def hello(name, number):\n",
    "    return f'hello {name} {number}'\n",
    "\n",
    "\n",
    "def test():\n",
    "    with FunctionExecutor() as fexec:\n",
    "        fut = fexec.call_async(hello, ('World', 1))\n",
    "        print(fut.result())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello World 1\n"
     ]
    }
   ],
   "source": [
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "DB_FILE_NAME = 'mydb.db'\n",
    "TEMP_FOLDER = './mapreducetemp'\n",
    "FINAL_FOLDER = './mapreducefinal'\n",
    "NUM_OF_RECORDS = 10\n",
    "TEMP_RESULTS_TBL = 'temp_results'\n",
    "BUCKET_NAME = \"cloud-object-storage-8k-cos-standard-7nq\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "## Serverless MapReduceEngine\n",
    "\n",
    "Modify MapReduceEngine from homework 2 into the MapReduceServerlessEngine where map and reduce tasks executed as a serverless actions, instead of local threads. In particular:\n",
    "1. Deploy all map tasks as a serverless actions by using Lithops against IBM Cloud Functions.\n",
    "2. Collect results from all map tasks and store them in the same SQLite as you used in MapReduceEngine and use the same code for the sort and shuffle phase.\n",
    "3. Deploy reduce tasks by using Lithops against IBM Cloud Functions. Instead of persisting results from reduce tasks, return results back to the MapReduceServerlessEngine and proceed with the same workflow as in MapReduceEngine\n",
    "4. Return results of reduce tasks to the user\n",
    "\n",
    "**Please attach:**  \n",
    "Text file with all log messages Lithops printed to console during the execution. Make\n",
    "sure log level is set to DEBUG mode.\n",
    "\n",
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished creating MyCSV0.csv\n",
      "finished creating MyCSV1.csv\n",
      "finished creating MyCSV2.csv\n",
      "finished creating MyCSV3.csv\n",
      "finished creating MyCSV4.csv\n",
      "finished creating MyCSV5.csv\n",
      "finished creating MyCSV6.csv\n",
      "finished creating MyCSV7.csv\n",
      "finished creating MyCSV8.csv\n",
      "finished creating MyCSV9.csv\n",
      "finished creating MyCSV10.csv\n",
      "finished creating MyCSV11.csv\n",
      "finished creating MyCSV12.csv\n",
      "finished creating MyCSV13.csv\n",
      "finished creating MyCSV14.csv\n",
      "finished creating MyCSV15.csv\n",
      "finished creating MyCSV16.csv\n",
      "finished creating MyCSV17.csv\n",
      "finished creating MyCSV18.csv\n",
      "finished creating MyCSV19.csv\n"
     ]
    }
   ],
   "source": [
    "input_data = []\n",
    "\n",
    "\n",
    "def seeder(number):\n",
    "    firstname = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "    city = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto', 'TelAviv', 'Kiev', 'Hamburg']\n",
    "    secondname = []\n",
    "    for i in range(10):\n",
    "        rand_name = names.get_last_name()\n",
    "        secondname.append(rand_name)\n",
    "    df = pd.DataFrame()\n",
    "    df[\"firstname\"] = np.random.choice(firstname, NUM_OF_RECORDS)\n",
    "    df[\"secondname\"] = np.random.choice(secondname, NUM_OF_RECORDS)\n",
    "    df[\"city\"] = np.random.choice(city, NUM_OF_RECORDS)\n",
    "    #     df[\"id\"] = df.index + 1\n",
    "    curr_file_name = str('MyCSV%s.csv' % number)\n",
    "    df.to_csv(curr_file_name, index=False)\n",
    "    print(\"finished creating MyCSV%s.csv\" % number)\n",
    "    input_data.append('./' + curr_file_name)\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    seeder(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder(s) already exist(s): [Errno 17] File exists: './mapreducetemp'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(TEMP_FOLDER)\n",
    "    os.mkdir(FINAL_FOLDER)\n",
    "except Exception as e:\n",
    "    print(f\"folder(s) already exist(s): {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "sql_create_temp_results_table = \"\"\"CREATE TABLE IF NOT EXISTS temp_results (\n",
    "                                    key text,\n",
    "                                    value text\n",
    "                                    ); \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "sql_group_by_key = \"\"\"SELECT key, GROUP_CONCAT(value)\n",
    "                      FROM temp_results GROUP BY key ORDER BY (key);\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "sql_drop_all_tables = \"\"\"DROP TABLE temp_results;\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [],
   "source": [
    "def drop_temp_tables(conn):\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(sql_drop_all_tables)\n",
    "    except Error as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to a SQLite database \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print(sqlite3.version)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return conn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "def create_table(conn, create_table_sql):\n",
    "    \"\"\" create a table from the create_table_sql statement\n",
    "    :param conn: Connection object\n",
    "    :param create_table_sql: a CREATE TABLE statement\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(create_table_sql)\n",
    "    except Error as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [
    "def get_grouped_values(conn):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql_group_by_key)\n",
    "\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    return rows"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [],
   "source": [
    "class MapReduceServerlessEngine:\n",
    "    def execute(self, input_data, map_function, reduce_function, params):\n",
    "        curr_map = 0\n",
    "\n",
    "        for map_document_path in input_data:\n",
    "            with FunctionExecutor() as fexec:\n",
    "                with open(map_document_path, 'r') as curr_file:\n",
    "                    fut = fexec.call_async(func=map_function, data=(curr_file, params['column'], map_document_path))\n",
    "                    map_result = fut.result()\n",
    "                    print(map_result)\n",
    "\n",
    "                    if map_result is not None:\n",
    "                        map_result_df = pd.DataFrame(map_result, columns=[\"key\", \"value\"])\n",
    "                        map_result_df.to_csv(TEMP_FOLDER + '/part-tmp-%s.csv' % str(curr_map), index=False, header=True)\n",
    "            curr_map += 1\n",
    "\n",
    "        for temp_file_name in os.scandir(TEMP_FOLDER):\n",
    "            csv_df = pd.read_csv(temp_file_name.path)\n",
    "            csv_df.to_sql(TEMP_RESULTS_TBL, connection, if_exists='append', index=False)\n",
    "\n",
    "        grouped_values = get_grouped_values(connection)\n",
    "\n",
    "        curr_reduce = 0\n",
    "        for reduce_value in grouped_values:\n",
    "            with FunctionExecutor() as fexec:\n",
    "                fut = fexec.call_async(reduce_function, (reduce_value[0], reduce_value[1]))\n",
    "                reduce_result = fut.result()\n",
    "                print(reduce_result)\n",
    "\n",
    "                if reduce_result is not None:\n",
    "                    result_df = pd.DataFrame(reduce_result, columns=[\"values\"])\n",
    "                    result_df.to_csv(FINAL_FOLDER + '/part-%s-final.csv' % str(curr_reduce), index=False, header=True)\n",
    "            curr_reduce += 1\n",
    "\n",
    "        return \"MapReduce Completed\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "def inverted_map(document_buffer, column_index, document_name):\n",
    "    values = pd.read_csv(filepath_or_buffer=document_buffer, usecols=[column_index], skiprows=1)\n",
    "\n",
    "    return [(x[0], document_name) for x in values.to_records(index=False)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "def inverted_reduce(value, documents):\n",
    "    ret_val = [value]\n",
    "    temp_set = set(documents.split(','))\n",
    "    ret_val.extend(temp_set)\n",
    "\n",
    "    return ret_val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "connection = create_connection(DB_FILE_NAME)\n",
    "create_table(connection, sql_create_temp_results_table)\n",
    "if connection is not None:\n",
    "    # create temp_results table\n",
    "    create_table(connection, sql_create_temp_results_table)\n",
    "else:\n",
    "    print(\"Error! cannot create the database connection.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Michael', './MyCSV0.csv'), ('Johanna', './MyCSV0.csv'), ('Steven', './MyCSV0.csv'), ('Albert', './MyCSV0.csv'), ('Scott', './MyCSV0.csv'), ('John', './MyCSV0.csv'), ('Albert', './MyCSV0.csv'), ('Steven', './MyCSV0.csv'), ('Marc', './MyCSV0.csv')]\n",
      "[('Marc', './MyCSV1.csv'), ('John', './MyCSV1.csv'), ('Albert', './MyCSV1.csv'), ('Albert', './MyCSV1.csv'), ('Johanna', './MyCSV1.csv'), ('Johanna', './MyCSV1.csv'), ('Scott', './MyCSV1.csv'), ('Michael', './MyCSV1.csv'), ('Marc', './MyCSV1.csv')]\n",
      "[('Scott', './MyCSV2.csv'), ('Albert', './MyCSV2.csv'), ('Scott', './MyCSV2.csv'), ('Marc', './MyCSV2.csv'), ('Johanna', './MyCSV2.csv'), ('Albert', './MyCSV2.csv'), ('John', './MyCSV2.csv'), ('Dana', './MyCSV2.csv'), ('Marc', './MyCSV2.csv')]\n",
      "[('Steven', './MyCSV3.csv'), ('Steven', './MyCSV3.csv'), ('Michael', './MyCSV3.csv'), ('Marc', './MyCSV3.csv'), ('Dana', './MyCSV3.csv'), ('Marc', './MyCSV3.csv'), ('John', './MyCSV3.csv'), ('Steven', './MyCSV3.csv'), ('Steven', './MyCSV3.csv')]\n",
      "[('Michael', './MyCSV4.csv'), ('Dana', './MyCSV4.csv'), ('Dana', './MyCSV4.csv'), ('Dana', './MyCSV4.csv'), ('Marc', './MyCSV4.csv'), ('Michael', './MyCSV4.csv'), ('Scott', './MyCSV4.csv'), ('Dana', './MyCSV4.csv'), ('Albert', './MyCSV4.csv')]\n",
      "[('Albert', './MyCSV5.csv'), ('Steven', './MyCSV5.csv'), ('Steven', './MyCSV5.csv'), ('Dana', './MyCSV5.csv'), ('Michael', './MyCSV5.csv'), ('Marc', './MyCSV5.csv'), ('Scott', './MyCSV5.csv'), ('Michael', './MyCSV5.csv'), ('Johanna', './MyCSV5.csv')]\n",
      "[('Johanna', './MyCSV6.csv'), ('Marc', './MyCSV6.csv'), ('Albert', './MyCSV6.csv'), ('Steven', './MyCSV6.csv'), ('Marc', './MyCSV6.csv'), ('Michael', './MyCSV6.csv'), ('Dana', './MyCSV6.csv'), ('Steven', './MyCSV6.csv'), ('Dana', './MyCSV6.csv')]\n",
      "[('John', './MyCSV7.csv'), ('Johanna', './MyCSV7.csv'), ('Marc', './MyCSV7.csv'), ('John', './MyCSV7.csv'), ('Johanna', './MyCSV7.csv'), ('Albert', './MyCSV7.csv'), ('Scott', './MyCSV7.csv'), ('Albert', './MyCSV7.csv'), ('Johanna', './MyCSV7.csv')]\n",
      "[('Scott', './MyCSV8.csv'), ('Albert', './MyCSV8.csv'), ('Johanna', './MyCSV8.csv'), ('Steven', './MyCSV8.csv'), ('Steven', './MyCSV8.csv'), ('Steven', './MyCSV8.csv'), ('Michael', './MyCSV8.csv'), ('Albert', './MyCSV8.csv'), ('Dana', './MyCSV8.csv')]\n",
      "[('Marc', './MyCSV9.csv'), ('Marc', './MyCSV9.csv'), ('Marc', './MyCSV9.csv'), ('John', './MyCSV9.csv'), ('Marc', './MyCSV9.csv'), ('Steven', './MyCSV9.csv'), ('John', './MyCSV9.csv'), ('Scott', './MyCSV9.csv'), ('Scott', './MyCSV9.csv')]\n",
      "[('Johanna', './MyCSV10.csv'), ('Albert', './MyCSV10.csv'), ('Steven', './MyCSV10.csv'), ('Steven', './MyCSV10.csv'), ('Dana', './MyCSV10.csv'), ('Dana', './MyCSV10.csv'), ('Steven', './MyCSV10.csv'), ('Steven', './MyCSV10.csv'), ('Steven', './MyCSV10.csv')]\n",
      "[('Johanna', './MyCSV11.csv'), ('Marc', './MyCSV11.csv'), ('Michael', './MyCSV11.csv'), ('Dana', './MyCSV11.csv'), ('Steven', './MyCSV11.csv'), ('Albert', './MyCSV11.csv'), ('Michael', './MyCSV11.csv'), ('Albert', './MyCSV11.csv'), ('Steven', './MyCSV11.csv')]\n",
      "[('John', './MyCSV12.csv'), ('Steven', './MyCSV12.csv'), ('Dana', './MyCSV12.csv'), ('Steven', './MyCSV12.csv'), ('Steven', './MyCSV12.csv'), ('John', './MyCSV12.csv'), ('Dana', './MyCSV12.csv'), ('Steven', './MyCSV12.csv'), ('Dana', './MyCSV12.csv')]\n",
      "[('Michael', './MyCSV13.csv'), ('John', './MyCSV13.csv'), ('Steven', './MyCSV13.csv'), ('Steven', './MyCSV13.csv'), ('Steven', './MyCSV13.csv'), ('Steven', './MyCSV13.csv'), ('Marc', './MyCSV13.csv'), ('Albert', './MyCSV13.csv'), ('Scott', './MyCSV13.csv')]\n",
      "[('Albert', './MyCSV14.csv'), ('Marc', './MyCSV14.csv'), ('Marc', './MyCSV14.csv'), ('Dana', './MyCSV14.csv'), ('Marc', './MyCSV14.csv'), ('Marc', './MyCSV14.csv'), ('Dana', './MyCSV14.csv'), ('Johanna', './MyCSV14.csv'), ('Scott', './MyCSV14.csv')]\n",
      "[('Marc', './MyCSV15.csv'), ('Michael', './MyCSV15.csv'), ('Dana', './MyCSV15.csv'), ('Dana', './MyCSV15.csv'), ('Steven', './MyCSV15.csv'), ('Albert', './MyCSV15.csv'), ('Johanna', './MyCSV15.csv'), ('Steven', './MyCSV15.csv'), ('Albert', './MyCSV15.csv')]\n",
      "[('John', './MyCSV16.csv'), ('Scott', './MyCSV16.csv'), ('Steven', './MyCSV16.csv'), ('Scott', './MyCSV16.csv'), ('Marc', './MyCSV16.csv'), ('Albert', './MyCSV16.csv'), ('Steven', './MyCSV16.csv'), ('John', './MyCSV16.csv'), ('Albert', './MyCSV16.csv')]\n",
      "[('John', './MyCSV17.csv'), ('John', './MyCSV17.csv'), ('Steven', './MyCSV17.csv'), ('Steven', './MyCSV17.csv'), ('Johanna', './MyCSV17.csv'), ('Scott', './MyCSV17.csv'), ('John', './MyCSV17.csv'), ('Johanna', './MyCSV17.csv'), ('Steven', './MyCSV17.csv')]\n",
      "[('Marc', './MyCSV18.csv'), ('Johanna', './MyCSV18.csv'), ('John', './MyCSV18.csv'), ('Marc', './MyCSV18.csv'), ('Marc', './MyCSV18.csv'), ('Scott', './MyCSV18.csv'), ('Michael', './MyCSV18.csv'), ('Albert', './MyCSV18.csv'), ('Dana', './MyCSV18.csv')]\n",
      "[('John', './MyCSV19.csv'), ('Albert', './MyCSV19.csv'), ('John', './MyCSV19.csv'), ('Dana', './MyCSV19.csv'), ('Steven', './MyCSV19.csv'), ('Marc', './MyCSV19.csv'), ('Albert', './MyCSV19.csv'), ('Steven', './MyCSV19.csv'), ('Dana', './MyCSV19.csv')]\n",
      "['Albert', 'input/MyCsv11.csv', 'input/MyCsv19.csv', 'input/MyCsv2.csv', 'input/MyCsv10.csv', './MyCSV10.csv', 'input/MyCsv18.csv', 'input/MyCsv17.csv', './MyCSV0.csv', 'input/MyCsv16.csv', 'input/MyCsv6.csv', './MyCSV4.csv', 'input/MyCsv3.csv', 'input/MyCsv15.csv', './MyCSV13.csv', './MyCSV6.csv', 'input/MyCsv4.csv', 'input/MyCsv9.csv', 'input/MyCsv14.csv', 'input/MyCsv8.csv', './MyCSV14.csv', 'input/MyCsv1.csv', './MyCSV16.csv', './MyCSV11.csv', 'input/MyCsv7.csv', './MyCSV18.csv', './MyCSV2.csv', './MyCSV15.csv', './MyCSV19.csv', './MyCSV7.csv', 'input/MyCsv0.csv', 'input/MyCsv5.csv', './MyCSV8.csv', './MyCSV5.csv', './MyCSV1.csv']\n",
      "['Dana', 'input/MyCsv19.csv', 'input/MyCsv10.csv', './MyCSV10.csv', 'input/MyCsv17.csv', 'input/MyCsv16.csv', 'input/MyCsv13.csv', './MyCSV3.csv', 'input/MyCsv6.csv', './MyCSV4.csv', 'input/MyCsv3.csv', 'input/MyCsv15.csv', './MyCSV6.csv', 'input/MyCsv4.csv', 'input/MyCsv14.csv', 'input/MyCsv8.csv', './MyCSV14.csv', './MyCSV11.csv', './MyCSV18.csv', './MyCSV2.csv', './MyCSV15.csv', './MyCSV19.csv', 'input/MyCsv0.csv', 'input/MyCsv5.csv', './MyCSV8.csv', './MyCSV12.csv', './MyCSV5.csv', 'input/MyCsv12.csv']\n",
      "['Johanna', './MyCSV1.csv', 'input/MyCsv11.csv', 'input/MyCsv19.csv', 'input/MyCsv2.csv', 'input/MyCsv10.csv', './MyCSV10.csv', 'input/MyCsv18.csv', 'input/MyCsv17.csv', './MyCSV17.csv', './MyCSV0.csv', 'input/MyCsv6.csv', 'input/MyCsv3.csv', 'input/MyCsv15.csv', './MyCSV6.csv', 'input/MyCsv4.csv', 'input/MyCsv9.csv', 'input/MyCsv14.csv', 'input/MyCsv8.csv', './MyCSV14.csv', 'input/MyCsv1.csv', './MyCSV11.csv', 'input/MyCsv7.csv', './MyCSV18.csv', './MyCSV2.csv', './MyCSV15.csv', './MyCSV7.csv', 'input/MyCsv0.csv', './MyCSV8.csv', './MyCSV5.csv', 'input/MyCsv12.csv']\n",
      "['John', './MyCSV1.csv', 'input/MyCsv11.csv', 'input/MyCsv2.csv', 'input/MyCsv18.csv', 'input/MyCsv17.csv', './MyCSV17.csv', './MyCSV0.csv', 'input/MyCsv16.csv', 'input/MyCsv13.csv', './MyCSV3.csv', 'input/MyCsv6.csv', './MyCSV13.csv', 'input/MyCsv4.csv', 'input/MyCsv9.csv', 'input/MyCsv8.csv', './MyCSV16.csv', 'input/MyCsv1.csv', 'input/MyCsv7.csv', './MyCSV18.csv', './MyCSV2.csv', './MyCSV9.csv', './MyCSV19.csv', './MyCSV7.csv', 'input/MyCsv5.csv', './MyCSV12.csv', 'input/MyCsv12.csv']\n",
      "['Marc', './MyCSV1.csv', 'input/MyCsv11.csv', 'input/MyCsv19.csv', 'input/MyCsv2.csv', 'input/MyCsv10.csv', 'input/MyCsv18.csv', 'input/MyCsv17.csv', './MyCSV0.csv', 'input/MyCsv16.csv', 'input/MyCsv13.csv', './MyCSV3.csv', './MyCSV4.csv', 'input/MyCsv3.csv', './MyCSV13.csv', './MyCSV6.csv', 'input/MyCsv4.csv', 'input/MyCsv9.csv', 'input/MyCsv14.csv', './MyCSV16.csv', './MyCSV14.csv', 'input/MyCsv1.csv', './MyCSV11.csv', './MyCSV18.csv', './MyCSV2.csv', './MyCSV15.csv', './MyCSV9.csv', './MyCSV19.csv', './MyCSV7.csv', 'input/MyCsv0.csv', 'input/MyCsv5.csv', './MyCSV5.csv', 'input/MyCsv12.csv']\n",
      "['Michael', 'input/MyCsv11.csv', 'input/MyCsv19.csv', 'input/MyCsv2.csv', 'input/MyCsv10.csv', 'input/MyCsv17.csv', './MyCSV0.csv', 'input/MyCsv16.csv', 'input/MyCsv13.csv', './MyCSV3.csv', 'input/MyCsv6.csv', './MyCSV4.csv', 'input/MyCsv3.csv', 'input/MyCsv15.csv', './MyCSV13.csv', './MyCSV6.csv', 'input/MyCsv4.csv', 'input/MyCsv9.csv', 'input/MyCsv1.csv', './MyCSV11.csv', 'input/MyCsv7.csv', './MyCSV18.csv', './MyCSV15.csv', 'input/MyCsv5.csv', './MyCSV8.csv', './MyCSV5.csv', './MyCSV1.csv']\n",
      "['Scott', './MyCSV1.csv', 'input/MyCsv11.csv', 'input/MyCsv19.csv', 'input/MyCsv2.csv', 'input/MyCsv18.csv', 'input/MyCsv17.csv', './MyCSV17.csv', './MyCSV0.csv', 'input/MyCsv16.csv', 'input/MyCsv6.csv', './MyCSV4.csv', 'input/MyCsv15.csv', './MyCSV13.csv', 'input/MyCsv4.csv', 'input/MyCsv9.csv', 'input/MyCsv8.csv', './MyCSV14.csv', 'input/MyCsv1.csv', './MyCSV16.csv', 'input/MyCsv7.csv', './MyCSV18.csv', './MyCSV2.csv', './MyCSV9.csv', './MyCSV7.csv', 'input/MyCsv0.csv', 'input/MyCsv5.csv', './MyCSV8.csv', './MyCSV5.csv', 'input/MyCsv12.csv']\n",
      "['Steven', 'input/MyCsv19.csv', 'input/MyCsv2.csv', './MyCSV10.csv', './MyCSV17.csv', './MyCSV0.csv', 'input/MyCsv16.csv', 'input/MyCsv13.csv', './MyCSV3.csv', 'input/MyCsv3.csv', 'input/MyCsv15.csv', './MyCSV13.csv', './MyCSV6.csv', 'input/MyCsv9.csv', 'input/MyCsv14.csv', 'input/MyCsv8.csv', './MyCSV16.csv', './MyCSV11.csv', './MyCSV15.csv', './MyCSV9.csv', './MyCSV19.csv', 'input/MyCsv0.csv', 'input/MyCsv5.csv', './MyCSV8.csv', './MyCSV12.csv', './MyCSV5.csv', 'input/MyCsv12.csv']\n",
      "MapReduce Completed\n"
     ]
    }
   ],
   "source": [
    "mapreduce = MapReduceServerlessEngine()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce, params={'column': 0})\n",
    "print(status)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "for file_name in os.scandir(TEMP_FOLDER):\n",
    "    os.remove(file_name.path)\n",
    "drop_temp_tables(connection)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Task 2\n",
    "## Submit MapReduce job to calculate inverted index\n",
    "1. Use input_data: `cos://bucket/<path to CSV data>`\n",
    "2. Submit MapReduce job with reduce and map functions as you used in homework 2, as follows\n",
    "\n",
    "    `mapreduce = MapReduceServerlessEngine()`  \n",
    "    `results = mapreduce.execute(input_data, inverted_map, inverted_index)`   \n",
    "    `print(results)`\n",
    "\n",
    "**Please attach:**  \n",
    "Text file with all log messages Lithops printed to console during the execution. Make\n",
    "sure log level is set to DEBUG mode.\n",
    "\n",
    "#### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "# Delete all result files in the final folder\n",
    "for file_name in os.scandir(FINAL_FOLDER):\n",
    "    os.remove(file_name.path)\n",
    "\n",
    "# Delete all .csv or .db files in the current directory\n",
    "for file_name in os.scandir('.'):\n",
    "    name, extension = os.path.splitext(file_name)\n",
    "    if extension == '.csv' or extension == '.db':\n",
    "        os.remove(file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "def upload_csv_to_bucket(path, key_name, csv_body):\n",
    "    try:\n",
    "        storage = Storage()\n",
    "        storage.put_object(bucket=BUCKET_NAME, key='input/' + str(key_name), body=csv_body)\n",
    "        print(f\"Uploaded: {path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Unable to create text file: {0}\".format(e))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "def seeder(number):\n",
    "    firstname = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "    city = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto', 'TelAviv', 'Kiev', 'Hamburg']\n",
    "    secondname = []\n",
    "    for i in range(10):\n",
    "        rand_name = names.get_last_name()\n",
    "        secondname.append(rand_name)\n",
    "    df = pd.DataFrame()\n",
    "    df[\"firstname\"] = np.random.choice(firstname, NUM_OF_RECORDS)\n",
    "    df[\"secondname\"] = np.random.choice(secondname, NUM_OF_RECORDS)\n",
    "    df[\"city\"] = np.random.choice(city, NUM_OF_RECORDS)\n",
    "\n",
    "    # Writing the generated DataFrame to a csv file\n",
    "    csv_str = df.to_csv(path_or_buf=None, index=False)\n",
    "\n",
    "    abspath = os.path.abspath('./MyCsv%s.csv' % number)\n",
    "    key_name = os.path.basename(abspath)\n",
    "\n",
    "    # Uploading the csv file to the bucket\n",
    "    upload_csv_to_bucket(abspath, key_name, csv_str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv0.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv1.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv2.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv3.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv4.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv5.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv6.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv7.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv8.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv9.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv10.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv11.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv12.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv13.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv14.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv15.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv16.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv17.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv18.csv\n",
      "Uploaded: /Users/rludan/git/BigDataHW3/MyCsv19.csv\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    seeder(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder(s) already exist(s): [Errno 17] File exists: './mapreducetemp'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(TEMP_FOLDER)\n",
    "    os.mkdir(FINAL_FOLDER)\n",
    "except Exception as e:\n",
    "    print(f\"folder(s) already exist(s): {e}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "sql_create_temp_results_table = \"\"\"CREATE TABLE IF NOT EXISTS temp_results (\n",
    "                                    key text,\n",
    "                                    value text\n",
    "                                    ); \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "sql_group_by_key = \"\"\"SELECT key, GROUP_CONCAT(value)\n",
    "                      FROM temp_results GROUP BY key ORDER BY (key);\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "sql_drop_all_tables = \"\"\"DROP TABLE temp_results;\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "def drop_temp_tables(conn):\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(sql_drop_all_tables)\n",
    "    except Error as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to a SQLite database \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print(sqlite3.version)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return conn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "def create_table(conn, create_table_sql):\n",
    "    \"\"\" create a table from the create_table_sql statement\n",
    "    :param conn: Connection object\n",
    "    :param create_table_sql: a CREATE TABLE statement\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(create_table_sql)\n",
    "    except Error as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def get_grouped_values(conn):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql_group_by_key)\n",
    "\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    return rows"
   ],
   "execution_count": 209,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class MapReduceServerlessEngine():\n",
    "    def execute(self, input_data, map_function, reduce_function, params):\n",
    "        curr_map = 0\n",
    "\n",
    "        for input_key in input_data:\n",
    "            with FunctionExecutor() as fexec:\n",
    "                fut = fexec.call_async(func=map_function, data=(input_key, params['column']))  # {\"key\":key, \"col\":0}\n",
    "                map_result = fut.result()\n",
    "                print(map_result)\n",
    "\n",
    "                if map_result is not None:\n",
    "                    map_result_df = pd.DataFrame(map_result, columns=[\"key\", \"value\"])\n",
    "                    map_result_df.to_csv(TEMP_FOLDER + '/part-tmp-%s.csv' % str(curr_map), index=False, header=True)\n",
    "            curr_map += 1\n",
    "\n",
    "        for temp_file_name in os.scandir(TEMP_FOLDER):\n",
    "            csv_df = pd.read_csv(temp_file_name.path)\n",
    "            csv_df.to_sql(TEMP_RESULTS_TBL, connection, if_exists='append', index=False)\n",
    "\n",
    "        grouped_values = get_grouped_values(connection)\n",
    "\n",
    "        curr_reduce = 0\n",
    "        for reduce_value in grouped_values:\n",
    "            with FunctionExecutor() as fexec:\n",
    "                fut = fexec.call_async(reduce_function, (reduce_value[0], reduce_value[1]))\n",
    "                reduce_result = fut.result()\n",
    "                print(reduce_result)\n",
    "\n",
    "                if reduce_result is not None:\n",
    "                    result_df = pd.DataFrame(reduce_result, columns=[\"values\"])\n",
    "                    result_df.to_csv(FINAL_FOLDER + '/part-%s-final.csv' % curr_reduce, index=False, header=True)\n",
    "            curr_reduce += 1\n",
    "\n",
    "        return \"MapReduce Completed\""
   ],
   "execution_count": 210,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def inverted_map(key, col):\n",
    "    storage = Storage()\n",
    "    buffer = storage.get_object(BUCKET_NAME, key, stream=True)\n",
    "\n",
    "    values = pd.read_csv(filepath_or_buffer=buffer, usecols=[col], skiprows=1)\n",
    "\n",
    "    return [(x[0], key) for x in values.to_records(index=False)]"
   ],
   "execution_count": 211,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def inverted_reduce(value, documents):\n",
    "    ret_val = [value]\n",
    "    temp_set = set(documents.split(','))\n",
    "    ret_val.extend(temp_set)\n",
    "\n",
    "    return ret_val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 212,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input/MyCsv0.csv', 'input/MyCsv1.csv', 'input/MyCsv10.csv', 'input/MyCsv11.csv', 'input/MyCsv12.csv', 'input/MyCsv13.csv', 'input/MyCsv14.csv', 'input/MyCsv15.csv', 'input/MyCsv16.csv', 'input/MyCsv17.csv', 'input/MyCsv18.csv', 'input/MyCsv19.csv', 'input/MyCsv2.csv', 'input/MyCsv3.csv', 'input/MyCsv4.csv', 'input/MyCsv5.csv', 'input/MyCsv6.csv', 'input/MyCsv7.csv', 'input/MyCsv8.csv', 'input/MyCsv9.csv']\n"
     ]
    }
   ],
   "source": [
    "storage = Storage()\n",
    "input_data = storage.list_keys(BUCKET_NAME, prefix='input/')\n",
    "\n",
    "print(input_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "connection = create_connection(DB_FILE_NAME)\n",
    "create_table(connection, sql_create_temp_results_table)"
   ],
   "execution_count": 214,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "if connection is not None:\n",
    "    # create temp_results table\n",
    "    create_table(connection, sql_create_temp_results_table)\n",
    "else:\n",
    "    print(\"Error! cannot create the database connection.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'input/MyCsv0.csv'), ('Scott', 'input/MyCsv0.csv'), ('Steven', 'input/MyCsv0.csv'), ('Steven', 'input/MyCsv0.csv'), ('Steven', 'input/MyCsv0.csv'), ('Michael', 'input/MyCsv0.csv'), ('Scott', 'input/MyCsv0.csv'), ('Michael', 'input/MyCsv0.csv'), ('Steven', 'input/MyCsv0.csv')]\n",
      "[('Marc', 'input/MyCsv1.csv'), ('Michael', 'input/MyCsv1.csv'), ('Steven', 'input/MyCsv1.csv'), ('John', 'input/MyCsv1.csv'), ('Michael', 'input/MyCsv1.csv'), ('Johanna', 'input/MyCsv1.csv'), ('Scott', 'input/MyCsv1.csv'), ('Johanna', 'input/MyCsv1.csv'), ('Steven', 'input/MyCsv1.csv')]\n",
      "[('Michael', 'input/MyCsv10.csv'), ('Dana', 'input/MyCsv10.csv'), ('Dana', 'input/MyCsv10.csv'), ('Dana', 'input/MyCsv10.csv'), ('Albert', 'input/MyCsv10.csv'), ('Dana', 'input/MyCsv10.csv'), ('Scott', 'input/MyCsv10.csv'), ('Albert', 'input/MyCsv10.csv'), ('Scott', 'input/MyCsv10.csv')]\n",
      "[('Steven', 'input/MyCsv11.csv'), ('Marc', 'input/MyCsv11.csv'), ('Michael', 'input/MyCsv11.csv'), ('John', 'input/MyCsv11.csv'), ('Albert', 'input/MyCsv11.csv'), ('Johanna', 'input/MyCsv11.csv'), ('Dana', 'input/MyCsv11.csv'), ('Marc', 'input/MyCsv11.csv'), ('Dana', 'input/MyCsv11.csv')]\n",
      "[('Michael', 'input/MyCsv12.csv'), ('John', 'input/MyCsv12.csv'), ('Scott', 'input/MyCsv12.csv'), ('Scott', 'input/MyCsv12.csv'), ('Albert', 'input/MyCsv12.csv'), ('Michael', 'input/MyCsv12.csv'), ('Dana', 'input/MyCsv12.csv'), ('Steven', 'input/MyCsv12.csv'), ('Johanna', 'input/MyCsv12.csv')]\n",
      "[('Albert', 'input/MyCsv13.csv'), ('Dana', 'input/MyCsv13.csv'), ('Marc', 'input/MyCsv13.csv'), ('Scott', 'input/MyCsv13.csv'), ('Steven', 'input/MyCsv13.csv'), ('John', 'input/MyCsv13.csv'), ('Albert', 'input/MyCsv13.csv'), ('Johanna', 'input/MyCsv13.csv'), ('John', 'input/MyCsv13.csv')]\n",
      "[('Dana', 'input/MyCsv14.csv'), ('John', 'input/MyCsv14.csv'), ('Michael', 'input/MyCsv14.csv'), ('Scott', 'input/MyCsv14.csv'), ('Albert', 'input/MyCsv14.csv'), ('Steven', 'input/MyCsv14.csv'), ('Steven', 'input/MyCsv14.csv'), ('Steven', 'input/MyCsv14.csv'), ('Marc', 'input/MyCsv14.csv')]\n",
      "[('Steven', 'input/MyCsv15.csv'), ('Michael', 'input/MyCsv15.csv'), ('Albert', 'input/MyCsv15.csv'), ('Dana', 'input/MyCsv15.csv'), ('Dana', 'input/MyCsv15.csv'), ('John', 'input/MyCsv15.csv'), ('Johanna', 'input/MyCsv15.csv'), ('Johanna', 'input/MyCsv15.csv'), ('Albert', 'input/MyCsv15.csv')]\n",
      "[('Johanna', 'input/MyCsv16.csv'), ('Steven', 'input/MyCsv16.csv'), ('Marc', 'input/MyCsv16.csv'), ('Dana', 'input/MyCsv16.csv'), ('John', 'input/MyCsv16.csv'), ('Johanna', 'input/MyCsv16.csv'), ('Johanna', 'input/MyCsv16.csv'), ('Dana', 'input/MyCsv16.csv'), ('John', 'input/MyCsv16.csv')]\n",
      "[('Michael', 'input/MyCsv17.csv'), ('Michael', 'input/MyCsv17.csv'), ('Steven', 'input/MyCsv17.csv'), ('Albert', 'input/MyCsv17.csv'), ('Albert', 'input/MyCsv17.csv'), ('Steven', 'input/MyCsv17.csv'), ('Albert', 'input/MyCsv17.csv'), ('Michael', 'input/MyCsv17.csv'), ('John', 'input/MyCsv17.csv')]\n",
      "[('Marc', 'input/MyCsv18.csv'), ('Steven', 'input/MyCsv18.csv'), ('Steven', 'input/MyCsv18.csv'), ('Steven', 'input/MyCsv18.csv'), ('Dana', 'input/MyCsv18.csv'), ('John', 'input/MyCsv18.csv'), ('Johanna', 'input/MyCsv18.csv'), ('Scott', 'input/MyCsv18.csv'), ('John', 'input/MyCsv18.csv')]\n",
      "[('Michael', 'input/MyCsv19.csv'), ('Scott', 'input/MyCsv19.csv'), ('Albert', 'input/MyCsv19.csv'), ('Johanna', 'input/MyCsv19.csv'), ('Albert', 'input/MyCsv19.csv'), ('Albert', 'input/MyCsv19.csv'), ('Marc', 'input/MyCsv19.csv'), ('Dana', 'input/MyCsv19.csv'), ('Johanna', 'input/MyCsv19.csv')]\n",
      "[('Marc', 'input/MyCsv2.csv'), ('Albert', 'input/MyCsv2.csv'), ('Dana', 'input/MyCsv2.csv'), ('Michael', 'input/MyCsv2.csv'), ('Albert', 'input/MyCsv2.csv'), ('Dana', 'input/MyCsv2.csv'), ('Dana', 'input/MyCsv2.csv'), ('Steven', 'input/MyCsv2.csv'), ('Albert', 'input/MyCsv2.csv')]\n",
      "[('Steven', 'input/MyCsv3.csv'), ('Marc', 'input/MyCsv3.csv'), ('Marc', 'input/MyCsv3.csv'), ('Michael', 'input/MyCsv3.csv'), ('Albert', 'input/MyCsv3.csv'), ('Marc', 'input/MyCsv3.csv'), ('John', 'input/MyCsv3.csv'), ('Marc', 'input/MyCsv3.csv'), ('Dana', 'input/MyCsv3.csv')]\n",
      "[('Dana', 'input/MyCsv4.csv'), ('Michael', 'input/MyCsv4.csv'), ('Scott', 'input/MyCsv4.csv'), ('Marc', 'input/MyCsv4.csv'), ('Johanna', 'input/MyCsv4.csv'), ('Scott', 'input/MyCsv4.csv'), ('Dana', 'input/MyCsv4.csv'), ('Steven', 'input/MyCsv4.csv'), ('Marc', 'input/MyCsv4.csv')]\n",
      "[('Scott', 'input/MyCsv5.csv'), ('Michael', 'input/MyCsv5.csv'), ('Steven', 'input/MyCsv5.csv'), ('Dana', 'input/MyCsv5.csv'), ('Albert', 'input/MyCsv5.csv'), ('Marc', 'input/MyCsv5.csv'), ('Steven', 'input/MyCsv5.csv'), ('Steven', 'input/MyCsv5.csv'), ('Johanna', 'input/MyCsv5.csv')]\n",
      "[('Albert', 'input/MyCsv6.csv'), ('John', 'input/MyCsv6.csv'), ('Marc', 'input/MyCsv6.csv'), ('Scott', 'input/MyCsv6.csv'), ('John', 'input/MyCsv6.csv'), ('Johanna', 'input/MyCsv6.csv'), ('John', 'input/MyCsv6.csv'), ('Dana', 'input/MyCsv6.csv'), ('Scott', 'input/MyCsv6.csv')]\n",
      "[('John', 'input/MyCsv7.csv'), ('Johanna', 'input/MyCsv7.csv'), ('Steven', 'input/MyCsv7.csv'), ('Dana', 'input/MyCsv7.csv'), ('Scott', 'input/MyCsv7.csv'), ('Steven', 'input/MyCsv7.csv'), ('John', 'input/MyCsv7.csv'), ('Albert', 'input/MyCsv7.csv'), ('John', 'input/MyCsv7.csv')]\n",
      "[('Michael', 'input/MyCsv8.csv'), ('Dana', 'input/MyCsv8.csv'), ('Michael', 'input/MyCsv8.csv'), ('Dana', 'input/MyCsv8.csv'), ('Dana', 'input/MyCsv8.csv'), ('Johanna', 'input/MyCsv8.csv'), ('Scott', 'input/MyCsv8.csv'), ('Scott', 'input/MyCsv8.csv'), ('Steven', 'input/MyCsv8.csv')]\n",
      "[('Marc', 'input/MyCsv9.csv'), ('Johanna', 'input/MyCsv9.csv'), ('Marc', 'input/MyCsv9.csv'), ('Albert', 'input/MyCsv9.csv'), ('Dana', 'input/MyCsv9.csv'), ('Dana', 'input/MyCsv9.csv'), ('Scott', 'input/MyCsv9.csv'), ('John', 'input/MyCsv9.csv'), ('Dana', 'input/MyCsv9.csv')]\n",
      "['Albert', 'input/MyCsv17.csv', 'input/MyCsv9.csv', 'input/MyCsv5.csv', 'input/MyCsv11.csv', 'input/MyCsv14.csv', 'input/MyCsv19.csv', 'input/MyCsv2.csv', 'input/MyCsv10.csv', 'input/MyCsv13.csv', 'input/MyCsv6.csv', 'input/MyCsv7.csv', 'input/MyCsv3.csv', 'input/MyCsv15.csv', 'input/MyCsv12.csv']\n",
      "['Dana', 'input/MyCsv4.csv', 'input/MyCsv9.csv', 'input/MyCsv5.csv', 'input/MyCsv11.csv', 'input/MyCsv14.csv', 'input/MyCsv19.csv', 'input/MyCsv16.csv', 'input/MyCsv2.csv', 'input/MyCsv10.csv', 'input/MyCsv13.csv', 'input/MyCsv8.csv', 'input/MyCsv6.csv', 'input/MyCsv7.csv', 'input/MyCsv18.csv', 'input/MyCsv3.csv', 'input/MyCsv15.csv', 'input/MyCsv12.csv']\n",
      "['Johanna', 'input/MyCsv4.csv', 'input/MyCsv9.csv', 'input/MyCsv5.csv', 'input/MyCsv11.csv', 'input/MyCsv19.csv', 'input/MyCsv16.csv', 'input/MyCsv8.csv', 'input/MyCsv13.csv', 'input/MyCsv1.csv', 'input/MyCsv6.csv', 'input/MyCsv7.csv', 'input/MyCsv18.csv', 'input/MyCsv15.csv', 'input/MyCsv12.csv']\n",
      "['John', 'input/MyCsv0.csv', 'input/MyCsv17.csv', 'input/MyCsv9.csv', 'input/MyCsv11.csv', 'input/MyCsv14.csv', 'input/MyCsv16.csv', 'input/MyCsv13.csv', 'input/MyCsv1.csv', 'input/MyCsv6.csv', 'input/MyCsv7.csv', 'input/MyCsv18.csv', 'input/MyCsv3.csv', 'input/MyCsv15.csv', 'input/MyCsv12.csv']\n",
      "['Marc', 'input/MyCsv4.csv', 'input/MyCsv9.csv', 'input/MyCsv5.csv', 'input/MyCsv11.csv', 'input/MyCsv14.csv', 'input/MyCsv19.csv', 'input/MyCsv16.csv', 'input/MyCsv2.csv', 'input/MyCsv13.csv', 'input/MyCsv1.csv', 'input/MyCsv6.csv', 'input/MyCsv18.csv', 'input/MyCsv3.csv']\n",
      "['Michael', 'input/MyCsv0.csv', 'input/MyCsv4.csv', 'input/MyCsv17.csv', 'input/MyCsv5.csv', 'input/MyCsv11.csv', 'input/MyCsv14.csv', 'input/MyCsv19.csv', 'input/MyCsv2.csv', 'input/MyCsv10.csv', 'input/MyCsv8.csv', 'input/MyCsv1.csv', 'input/MyCsv3.csv', 'input/MyCsv15.csv', 'input/MyCsv12.csv']\n",
      "['Scott', 'input/MyCsv0.csv', 'input/MyCsv4.csv', 'input/MyCsv9.csv', 'input/MyCsv5.csv', 'input/MyCsv14.csv', 'input/MyCsv19.csv', 'input/MyCsv10.csv', 'input/MyCsv13.csv', 'input/MyCsv1.csv', 'input/MyCsv8.csv', 'input/MyCsv6.csv', 'input/MyCsv7.csv', 'input/MyCsv18.csv', 'input/MyCsv12.csv']\n",
      "['Steven', 'input/MyCsv0.csv', 'input/MyCsv4.csv', 'input/MyCsv17.csv', 'input/MyCsv5.csv', 'input/MyCsv11.csv', 'input/MyCsv14.csv', 'input/MyCsv16.csv', 'input/MyCsv2.csv', 'input/MyCsv8.csv', 'input/MyCsv13.csv', 'input/MyCsv1.csv', 'input/MyCsv7.csv', 'input/MyCsv18.csv', 'input/MyCsv3.csv', 'input/MyCsv15.csv', 'input/MyCsv12.csv']\n",
      "MapReduce Completed\n"
     ]
    }
   ],
   "source": [
    "mapreduce = MapReduceServerlessEngine()\n",
    "status = mapreduce.execute(input_data, inverted_map, inverted_reduce, params={'column': 0})\n",
    "print(status)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "for file_name in os.scandir(TEMP_FOLDER):\n",
    "    os.remove(file_name.path)\n",
    "drop_temp_tables(connection)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "## Shuffle\n",
    "\n",
    "MapReduceServerlessEngine deploys both map and reduce tasks as serverless invocations.   \n",
    "However, once map stage completed, the result are transferred from the map tasks to the SQLite database located on the client machine (laptop in your case), then performed local shuffle and then invoked reduce tasks passing them relevant parameters.\n",
    "\n",
    "(To support your answers, feel free to use examples, Images, etc.)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Explain why this approach is not efficient and what are cons and pros of such architecture in general. In broader scope you may assume that MapReduceServerlessEngine executed in some powerful machine and not just laptop.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "In general, moving from local to cloud, and local hardware to serverless has many advantages in speed, efficiency, fault-tolerance, and reliability, which is why more & more work has migrated in this direction ove the last 5-10 years. Using serverless invocations for both the Map and Reduce tasks enables the use of cloud computing infrastructure, improving speed and efficiency and reducing the need for local storage.\n",
    "\n",
    "However in Step 1 we still run the shuffle step locally, on the SQL database located on our machine. This is a crucial step in MapReduce, and downloading all that data from the cloud to the local machine, performing the shuffle, and then invoking the Reduce step and sending the data back to the cloud, really compromises a lot of the benefits of running the Map & Reduce steps on the cloud. This can cause local memory issues and slow down the entire engine and output. Uploading all the data to the cloud and running the operations there, but then needing to download all the data to do the shuffle locally also just doesn't make a whole lot of sense from a workflow standpoint.\n",
    "\n",
    "### Pros:\n",
    "- don't need to pay as much, in theory, since the shuffle step is resource-intensive and is being run locally. However, this is only theory, as needing to download the data and run the shuffle locally could just as well end up costing more, either in direct costs of energy / bandwidth / compute, and/or indirect costs of affecting the performance of the local machine and/or other tasks.\n",
    "- Localizing the shuffle step may provide more control or degrees of freedom to customize in some phases of the engine, if cloud provider doesn't allow certain actions or parameters that are important and specific to the desired design of the shuffle.\n",
    "\n",
    "\n",
    "### Cons:\n",
    "- Big Data: the larger the amount of data being handled by the MapReduceEngine, the more strain the local shuffle step puts on the local machine, especially when it's a laptop as it is in our case. Even with the assumption of using some super-powerful local machine, the limits of the amount of data it can handle and the amount of bandwidth for downloading the data and \"sending back\", are all finite, and pale in comparison to the scalability of utilizing cloud computing infrastructure.\n",
    "- Failure & Fault-tolerance: regardless of the type of local machine, having the shuffle step handled locally increases the level of communication between the local machine and the cloud computing infrastructure, and more communication and coordination by default increases the possibilities and probabilities of encountering some failures along the way. This speaks to the \"cost\" savings we discussed above as a potential theoretical pro - as those savings could be trivial compared to the cost of failures.\n",
    "- Losing benefits of serverless invocations of Map & Reduce, as the overall complexity of the operation will be driven by the slowest part of the chain, meaning no matter how much we gain in efficiency and fault-tolerance by running Map & Reduce on the cloud, the limitations of performing the shuffle locally (and the required transfer of the data to & from the local server) can and likely will drown out the gains from the serverless execution of Map & Reduce. As can be seen in the diagram below, there is no way around the shuffle step - it will by default be a rate-limiting step...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Images:\n",
    "![image1](image1.png)\n",
    "![image2](image2.png)\n",
    "![image3](image3.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**2. Suggest how can you improve shuffle so intermediate data will not be downloaded to the client at all and shuffle performed in the cloud as well. Explain pros and cons of the approaches you suggest.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "The best way to improve the shuffle step and streamline the entire engine in the cloud will involve some version of localizing the mapped data database to the cloud (SQL or otherwise) in object storage (including a choice about whether it should be in the same bucket or a separate one), and modifying the shuffle function itself to conform to being a serverless action in the cloud environment, and to the specifications of the cloud provider.\n",
    "\n",
    "### Pros:\n",
    "- enables much greater potential scalability for larger projects and big data\n",
    "    - also enables using hybrid shuffle where slow and fast storage can be utilized when appropriate, and big-data shuffles can be partitioned into smaller chunks or rounds (*see image below).\n",
    "- no need to manage local hardware / infrastructure, equipment, processing power, and most importantly, storage\n",
    "- the entire engine is \"under one roof\", the data stays in one place, and any debugging and troubleshooting can be more efficient as it's all in the same common infrastructure.\n",
    "    - fault-tolerance would be more reliable as the cloud service provider certainly has backup covered\n",
    "- as a result, there is potential for a better cost-performance trade-off than would otherwise be achieved with the local shuffle\n",
    "\n",
    "### Cons:\n",
    "- potentially higher overall absolute cost. The relative cost could still be more worthwhile, but depending on the project and the data in question, a higher overall cost may be overkill and not the best use of resources\n",
    "- potentially decreased customizability\n",
    "- for very simple tasks in accessing the data, the latency would be higher and could take longer on the cloud when compared to running locally, for that very specific finite task.\n",
    "- increasing the burden on the serverless engine and adding more and more functions comes with its own risks, in communication and coordination of functions, and depending on the amount of and type of functions, and the serverless engine's capabilities and development, this could create additional unwanted complexity.\n",
    "- the fault-tolerance, error-rate, latency, and run-time would all be beholden to the functioning of the cloud infrastructure and servers. This can lead to situations that are out of the user's control compared to handling things on a local server/machine and having full control and transparency into everything."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Images:\n",
    "\n",
    "![image4.png](image4.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sources:\n",
    "\n",
    "- https://www.researchgate.net/publication/338040188_REDUCING_DATA_SHUFFLING_AND_IMPROVING_MAP_REDUCE_PERFORMANCE_USING_ENHANCED_DATA_LOCALITY\n",
    "\n",
    "- http://norma.ncirl.ie/4147/1/achyutanantakumarvadavadagi.pdf\n",
    "\n",
    "- https://www.usenix.org/system/files/nsdi19-pu.pdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "**3. Can you make serverless shuffle?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, serverless shuffle IS possible. Can be done by creating an SQL database in object storage and then performing the shuffle in a similiar but serverless-conforming function. Alternatively, we could bypass SQL and write code that will handle the mapped data and perform the same shuffle & sort functionality. This can be done using the lithops call_async method (see references below) to dramatically simplify and reduce steps required. However, it is not a slam-dunk that this would be worthwhile, as it could consume more resources than are absolutely necessary, and also because of the shuffle being so critical to MapReduce and being inter-connected to both Map & Reduce sides, there is potential to complicate the workflow and reduce the efficiency of the overall engine, depending on a lot of other factors.\n",
    "\n",
    "#### Lithops call_async references:\n",
    "\n",
    "- https://github.com/lithops-cloud/lithops/blob/master/examples/call_async.py)\n",
    "- https://www.ibm.com/cloud/blog/serverless-without-constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "Good Luck :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}